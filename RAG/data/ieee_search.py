from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
import os
import json

def search_and_download_streaming(keyword, save_dir, downloaded_links, cache_file, max_results=10):
    """
    Stream processing: search one page, download immediately.
    Eliminates intermediate storage, processes papers as they're found.
    
    About search links
    - Link copy from search web
        - https://www.ieee.org/search-results?q=ship+detection#gsc.tab=0&gsc.q=ship%20detection&gsc.page=1  
        - Below one generated by AI
    """
    paper_per_page = 10
    num_of_pages = max_results // paper_per_page if max_results >= paper_per_page else 1
    
    options = Options()
    # options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)
    
    downloaded_count = 0
    skipped_count = 0
    processed_count = 0
    
    try:
        for page in range(num_of_pages):
            if processed_count >= max_results:
                break
            url = f"https://www.ieee.org/search-results?q={keyword.replace(' ', '+')}&gsc.page={page+1}"
            print(f"Searching page {page+1}: {url}")
            driver.get(url)
            
            # Wait for results to load
            wait = WebDriverWait(driver, 20)
            try:
                wait.until(EC.presence_of_element_located((By.CLASS_NAME, "gs-title")))
            except:
                print(f"Timeout on page {page+1}, skipping")
                continue
            
            # Process each paper immediately
            paper_elements = driver.find_elements(By.CLASS_NAME, "gs-title")
            for paper_element in paper_elements:
                if processed_count >= max_results:
                    break
                    
                title = paper_element.text
                link = paper_element.get_attribute("href")
                
                if not (title and link):
                    continue
                    
                processed_count += 1
                print(f"\nProcessing paper {processed_count}: {title[:50]}...")
                
                # Check cache first
                if link in downloaded_links and os.path.exists(downloaded_links[link]):
                    print(f"Already cached: {os.path.basename(downloaded_links[link])}")
                    skipped_count += 1
                    continue
                
                # Download immediately
                result = download_paper(link, save_dir, downloaded_links, cache_file)
                if result:
                    downloaded_count += 1
                    print(f"Downloaded: {os.path.basename(result)}")
                else:
                    print("Download failed")
                    
    finally:
        driver.quit()
        
    return downloaded_count, skipped_count, processed_count

def load_downloaded_links(cache_file):
    """Load previously downloaded PDF links from cache file"""
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_downloaded_links(downloaded_links, cache_file):
    """Save downloaded PDF links to cache file"""
    os.makedirs(os.path.dirname(cache_file), exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump(downloaded_links, f, indent=2)

def download_paper(
    paper_url, 
    save_dir, 
    downloaded_links,
    cache_file
    ):
    
    """Something about the pdf url
    - If download from f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9984160" type of link
        it will not show pdf but download something include:
        <iframe 
        src="https://ieeexplore.ieee.org/ielx7/6046/8466674/08438999.pdf?tp=&arnumber=8438999&isnumber=8466674&ref=" 
        frameborder=0>
        </iframe>
    - This should be better: f"https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber={paper_id}"
    - Where and how do i get the pdf url? I dont know. Still trying to find it.
    """

    """Download and save IEEE paper PDF"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)    

    # Extract paper ID from IEEE URL
    # Handle different IEEE URL formats
    if 'arnumber=' in paper_url:
        paper_id = paper_url.split('arnumber=')[1].split('&')[0]
    elif '/document/' in paper_url:
        paper_id = paper_url.split('/document/')[1].split('/')[0]
    else:
        paper_id = paper_url.split('/')[-1] if paper_url.split('/')[-1].isdigit() else paper_url.split('/')[-2]
    
    pdf_url = f"https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber={paper_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    try:
        response = requests.get(pdf_url, headers=headers, timeout=10)
        if response.status_code == 200: #  and 'application/pdf' in response.headers.get('content-type', ''):
            filename = f"{paper_id}.pdf"
            filepath = os.path.join(save_dir, filename)
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            # Save to cache
            downloaded_links[paper_url] = filepath
            save_downloaded_links(downloaded_links, cache_file)
            
            print(f"Downloaded: {filename}")
            return filepath
    except Exception as e:
            print(f"Error downloading paper: {e}")
            return None

def search_and_download_papers(
    keyword, 
    save_dir="./ieee_papers/",
    cache_file="./ieee_papers/downloaded_links.json",
    max_results=2
    ):
    
    # Load downloaded links
    downloaded_links = load_downloaded_links(cache_file)
    
    print(f"Streaming search and download for '{keyword}' (max {max_results} papers)...")
    
    downloaded_count, skipped_count, processed_count = search_and_download_streaming(
        keyword, save_dir, downloaded_links, cache_file, max_results
    )
    
    print(f"\nSummary: {processed_count} papers processed, {downloaded_count} new downloads, {skipped_count} already cached")

if __name__ == "__main__":
    keyword = "ship detection"
    search_and_download_papers(
        keyword, 
        save_dir="./test_ieee_papers/", 
        cache_file="./test_ieee_papers/downloaded_links.json",
        max_results=5
        )